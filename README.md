# Auto-Prompting with a Multi-Agent System

This project demonstrates an **iterative prompt engineering workflow** using a multi-agent system built with LangGraph. The system automatically generates and refines prompts to achieve a specific goal, simulating a collaborative human-like process. It's designed to create high-quality, effective prompts for tasks like entity extraction from legal documents.

-----

### Features

  * **Automated Prompt Generation:** An AI agent (Worker 1) automatically creates a high-quality prompt based on a user-defined goal.
  * **Iterative Refinement Loop:** A second agent (Worker 2) evaluates the generated prompt and provides constructive feedback. The system then uses this feedback to improve the prompt in subsequent iterations.
  * **Expert Collaboration Simulation:** The workflow mimics the collaboration between a prompt designer and an evaluator, leading to more robust and effective prompts.
  * **Score-Based Termination:** The process continues until the prompt achieves a satisfactory evaluation score (\>= 8/10) or a maximum number of iterations is reached, preventing infinite loops.

-----

### How It Works

The system is built as a state graph with two primary agents (nodes) and a supervisor (conditional edge):

1.  **Worker 1: Prompt Generator**

      * Takes a **goal** (e.g., "extract entities from legal contracts") and any feedback from the previous round.
      * Generates a detailed and structured prompt using the LLM.

2.  **Worker 2: Prompt Evaluator**

      * Receives the prompt generated by Worker 1.
      * Evaluates the prompt for **clarity, specificity, and effectiveness**, assigning a score from 1 to 10.
      * If the score is below a threshold (e.g., 8), it generates **feedback** to help improve the prompt.

3.  **Supervisor:**

      * A conditional router that decides the next step.
      * If the prompt's score is **8 or higher**, or if the system has reached the maximum iteration count (5), the process **ends**.
      * Otherwise, the process loops back to **Worker 1** with the new feedback.

-----

### üõ†Ô∏è Tech Stack

  * **Frameworks:** LangChain, LangGraph
  * **LLMs:** AzureChatOpenAI (specifically `gpt-4o` for this example)
  * **Utilities:** `python-dotenv` for managing environment variables
  * **Language:** Python

-----

### Setup and Installation

1.  **Clone the repository:**

    ```bash
    git clone https://github.com/your-username/your-repo-name.git
    cd your-repo-name
    ```

2.  **Install dependencies:**

    ```bash
    pip install -r requirements.txt
    ```

3.  **Set up environment variables:**
    Create a `.env` file in the root directory and add your API keys. You'll need credentials for your Azure OpenAI instance.

    ```
    AZURE_OPENAI_API_KEY="your-api-key"
    AZURE_OPENAI_ENDPOINT="your-endpoint-url"
    ```

4.  **Run the script:**
    Execute the Python script or Jupyter notebook containing the LangGraph code. The `graph.invoke()` method will start the iterative process.

    ```python
    # Example usage
    final_state = graph.invoke({"goal": "Extract entities from legal contracts"})
    print("‚úÖ Final Prompt:\n", final_state["prompt"])
    print("\nüìù Feedback History:\n", final_state["history"])
    ```
